---
title: Vibe Learning
createdAt: 2026-01-22T11:53:30.939Z
updatedAt: 2026-01-22T11:53:30.939Z
id: 7b0043bc-6acd-404a-8678-09a213d2e211
shortTitle: ''
heroImage: ''
description: ''
# Valid tags: startups, business, writing, reading, ai, learning, education, philosophy, software, economics, personal, health, thinking
tags: []
slug: vibe-learning
---
My journey of learning to code coincided with the launch of ChatGPT. This wasn't planned, just a happy coincident. But that meant my journey was a bit different from the ones that came before me. I never used stack overflow. I googled and read the docs but soon searches were replaced by chatgpt, and over time reading the docs has mainly become reading the answers that LLMs give me when i feed them the docs or when i use a MCP server. 

The journey has been rewarding. The learning has been fast, at least that's what I think. But now I feel that I am the bottleneck. LLMs generate code far faster than what i can truly understand. I just can't seem to keep up with them. The pragmatic question then becomes - how do  I solve this problem. 

Let me frame the problem correctly. I am at a point where I can build faster than I can understand. I am in control of a giant swarm of agents whose work I can't trust. I don't even have the capability to understand their work. 

Can I read the code? Yes. But even with that, at some point of time, there comes a point where it is all a black box. So the question then becomes how do I ensure that the blackbox does what is says, does not fail when i don't want it to fail? 

Let me think of problems analogous to these 

### Principal Agent
I think there might be some relation of this to the principal agent problem. But for that, the incentives of the model should be different from my incentives. Maybe this is true. Maybe in some cases where the system prompt explicitly asks the agent to behave in the way I don't want it to behave. 


### Determinism
Maybe the issue is determinism? Programming has always been deterministic at the IC level. But that might no longer be the case? Maybe programming is now like real life - the outcome can be completely different from the intent. Before, I think, in programming complexity emerged out of interaction of a lot of deterministic parts because we assumed that these parts were stable. But now we are even going one step away form the parts being stable. 

How did we know that the parts were stable in the first place? Did manual human reviews cause the stability? Did the word of the engineer cause it? Did the automated tests cause it? What increased our confidence in the fact that things won't break? 

Maybe if we learn how to identify the stability of the things, we might understand what to learn? 


### Does it matter what is in the box?
Does it matter what is in the black box? How comfortable are we with uncertainty? How do we learn to trust the black box?  What do we need to learn to trust the black box?  
 
























